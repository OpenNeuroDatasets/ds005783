{\rtf1\ansi\ansicpg1252\cocoartf2709
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;\red26\green26\blue26;
}
{\*\expandedcolortbl;;\cssrgb\c0\c1\c1;\cssrgb\c100000\c100000\c100000\c0;\cssrgb\c13362\c13362\c13329;
}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs24 \cf2 \cb3 \expnd0\expndtw0\kerning0
17 adult participants (17 participants consented to open data sharing and are included here) watched short, ~3s video clips of the first-person perspective of walking through rooms. Rooms had a door on either the left, right, or both, and ego-motion was forward, backward, left or right. Code and stimuli descriptions shared here: https://osf.io/6yehp. We also scanned participants on a dynamic scene, face, and object localizer task.\
\
Main Experiment (\'93EXP\'94)\
Experimental stimuli consisted of 14 conditions (Conditions 1-10 are shown in Figure 2; Example stimuli and results from conditions 11-14 are shown in Supplemental Figure 3). All stimuli were created using Unity software and depicted 3 second clips of the first-person experience of walking through scenes. Navigational affordances were manipulated by including an open doorway to either the left side, right side, or both sides. To help control for low-level visual confounds, the non-doorway side always included a distractor object, either a painting (conditions 1-10) or an inverted doorway (conditions 11-14). Furthermore, the textures applied to the painting and the walls through the doorways were counterbalanced, such that each texture appeared equally on either side across the full stimulus set. Ego-motion was manipulated by changing the direction of ego-motion through scene, which could either be forward (conditions 1-3, 11-14), backward (conditions 4-6), a left turn (conditions 7-8) or a right turn (conditions 9-10). To help prevent visual adaptation over the course of the experiment, the 14 experimental conditions were counterbalanced across 8 room types, which differed from one another based on the textures applied to the walls, floor and ceiling, and to a lesser extent, by the size and shape of the doorways and corresponding distractor (Figure 5). Stimuli were presented at 13.1 x 18.6 DVA in an event-related paradigm. Each stimulus was presented for 2.5s, followed by a minimum inter-stimulus-interval (ISI) of 3.5s and a maximum ISI of 9.5s, optimized separately for each run using OptSeq2. Participants viewed 4 repetitions of each condition per run, and completed 8 experimental runs, yielding 32 total repetitions per condition across the experiment. To help ensure participants paid attention throughout the experiment, participants performed a one-back task, responding via button press whenever the exact same video stimulus repeated on back-to-back trials. Participants were also instructed to lie still, keep their eyes open, and try to \'93pay attention to\'94 and \'93immerse themselves in\'94 the stimuli.\
\
Localizer (\'93LOC\'94)\
Localizer stimuli consisted of 3s videos of dynamic Scenes, Objects, Faces, and Scrambled Objects, as described previously in Kamps et al., 2016 and 2020. Stimuli were presented using a block design at 13.7 x 18.1 degrees of visual angle. Each run was 315s long and contained 4 blocks per stimulus category. The order of the first set of blocks was pseudorandomized across runs (e.g., Faces, Objects, Scenes, Scrambled) and the order of the second set of blocks was the palindrome of the first (e.g., Scrambled, Scenes, Objects, Faces). Each block consisted of 5 2.8s video clips from a single condition, with an ISI of 0.2s, resulting in 15s blocks. Each run also included 5 fixation blocks: one at the beginning, three evenly spaced throughout the run, and one at the end. Participants completed 3 Localizer runs, interleaved between every 2 Experimental Runs.}